backend: http
thread_pool_max_workers: 64

mcp:
  transport: sse
  host: "0.0.0.0"
  port: 8002

http:
  host: "0.0.0.0"
  port: 8002
  timeout_keep_alive: 600
  limit_concurrency: 64

flow:
  retrieve_task_memory:
    flow_content: BuildQueryOp() >> RecallVectorStoreOp() >> RerankMemoryOp(enable_llm_rerank=False, enable_score_filter=False, top_k=5) >> RewriteMemoryOp(enable_llm_rewrite=False)
    description: "Retrieves the most relevant top-k memory experiences from historical data based on the current query to enhance task-solving capabilities"
    input_schema:
      query:
        type: string
        description: "user query"
        required: true

  summary_task_memory:
    flow_content: TrajectoryPreprocessOp(success_threshold=1.0) >> (SuccessExtractionOp()|FailureExtractionOp()|ComparativeExtractionOp(enable_soft_comparison=True)) >> MemoryValidationOp(validation_threshold=0.5) >> MemoryDeduplicationOp() >> UpdateVectorStoreOp()
    description: "Summarizes conversation trajectories or messages into structured memory representations for long-term storage"
    input_schema:
      trajectories:
        type: array
        description: "A list of conversation trajectory information, including message content and score. This field does not need to be filled in, the system will complete it automatically."
        required: false

  retrieve_task_memory_simple:
    flow_content: BuildQueryOp() >> RecallVectorStoreOp() >> MergeMemoryOp()
    description: "Retrieves the most relevant top-k memory experiences from historical data based on the current query to enhance task-solving capabilities"
    input_schema:
      query:
        type: string
        description: "user query"
        required: true

  summary_task_memory_simple:
    flow_content: SimpleSummaryOp() >> UpdateVectorStoreOp()
    description: "Summarizes conversation trajectories or messages into structured memory representations for long-term storage"
    input_schema:
      trajectories:
        type: array
        description: "A list of conversation trajectory information, including message content and score. This field does not need to be filled in, the system will complete it automatically."
        required: false

  retrieve_personal_memory:
    flow_content: SetQueryOp() >> (ExtractTimeOp() | (RetrieveMemoryOp() >> SemanticRankOp())) >> FuseRerankOp()
    description: "Retrieves the most relevant personal memories from historical data based on the query to enhance response quality"
    input_schema:
      query:
        type: string
        description: "user query"
        required: true

  summary_personal_memory:
    flow_content: InfoFilterOp() >> (GetObservationOp() | GetObservationWithTimeOp() | LoadTodayMemoryOp()) >> ContraRepeatOp() >> UpdateVectorStoreOp()
    description: "Consolidates user observations and memories by filtering information and removing redundancies for efficient storage"
    input_schema:
      trajectories:
        type: array
        description: "A list of conversation trajectory information, including message content and score. This field does not need to be filled in, the system will complete it automatically."
        required: false

  retrieve_tool_memory:
    flow_content: RetrieveToolMemoryOp()
    description: "Retrieves tool memories from the vector database based on tool names to provide tool usage patterns and best practices"
    input_schema:
      tool_names:
        type: string
        description: "Comma-separated tool names (e.g., 'tool_name1,tool_name2')"
        required: true

  add_tool_call_result:
    flow_content: ParseToolCallResultOp(max_history_tool_call_cnt=100, evaluation_sleep_interval=1.0) >> UpdateVectorStoreOp()
    description: "Evaluates and adds tool call results to the tool memory database, creating new memory or updating existing memory for the specified tool"
    input_schema:
      tool_call_results:
        type: array
        description: "List of tool call result objects, each containing: tool_name, input, output, success, time_cost, token_cost, create_time"
        required: true

  summary_tool_memory:
    flow_content: SummaryToolMemoryOp(recent_call_count=20, summary_sleep_interval=1.0) >> UpdateVectorStoreOp()
    description: "Analyzes tool call history and generates comprehensive usage patterns, best practices, and recommendations for the specified tools"
    input_schema:
      tool_names:
        type: string
        description: "Comma-separated tool names to summarize (e.g., 'tool_name1,tool_name2')"
        required: true

  use_mock_search:
    flow_content: UseMockSearchOp()
    description: "Simulates intelligent search tool selection and execution based on query complexity, with automatic tool memory recording"
    input_schema:
      query:
        type: string
        description: "User search query to process"
        required: true

  vector_store:
    flow_content: VectorStoreActionOp()
    description: "Directly operates on the vector store with various management actions"
    input_schema:
      action:
        type: string
        description: "vector store operations"
        required: true
        enum: [ copy, delete, delete_ids, dump, load, list]

  record_task_memory:
    flow_content: UpdateMemoryFreqOp() >> UpdateMemoryUtilityOp() >> UpdateVectorStoreOp()
    description: "Update the freq & utility attributes of retrieved task memories"
    input_schema:
      workspace_id:
        type: string
        description: "workspace id"
        required: true
      memory_dicts:
        type: array
        description: "A list of retrieved task memory corresponding to the current task."
        required: true
      update_utility:
        type: boolean
        description: "Whether to update the utility attribute of the retrieved task memory."
        required: true

  delete_task_memory:
    flow_content: DeleteMemoryOp() >> UpdateVectorStoreOp()
    description: "Delete task memories when utility/freq < utility_threshold and freq >= freq_threshold"
    input_schema:
      workspace_id:
        type: string
        description: "workspace id"
        required: true
      freq_threshold:
        type: integer
        description: "The retrieved frequency threshold for deleting task memory."
        required: true
      utility_threshold:
        type: number
        description: "The utility/freq threshold for deleting task memory."
        required: true

  react:
    flow_content: SimpleReactOp()
    description: "React to the current task with an agent"
    input_schema:
      query:
        type: string
        description: "user query"
        required: true

  agentic_retrieve:
    flow_content: AgenticRetrieveOp()

  summary_working_memory:
    flow_content: MessageOffloadOp() >> BatchWriteFileOp()
    description: "Summarizes working memory by compacting tool messages and compressing conversation history. First compacts large tool messages by storing full content in external files, then applies LLM-based compression if compaction ratio exceeds threshold. This helps reduce token usage while preserving important information."
    input_schema:
      messages:
        type: array
        description: "List of conversation messages to process for working memory summarization"
        required: true
      working_summary_mode:
        type: string
        description: "Working summary strategy: 'compact' only compacts large tool messages, 'compress' only applies LLM-based compression, 'auto' first compacts then optionally compresses when reduction is insufficient. Defaults to 'auto'."
        required: false
        enum: ["compact", "compress", "auto"]
      compact_ratio_threshold:
        type: number
        description: "Only used in 'auto' mode. Threshold for compaction ratio (tokens after compaction divided by original tokens). When the ratio is greater than this value, an additional LLM-based compression pass is triggered. Defaults to 0.75."
        required: false
      max_total_tokens:
        type: integer
        description: "Maximum token count threshold for triggering compression/compaction. For compaction, this is the total token count threshold. For compression, this excludes keep_recent_count messages and system messages. Defaults to 20000."
        required: false
      max_tool_message_tokens:
        type: integer
        description: "Maximum token count per tool message before compaction is applied. Tool messages exceeding this threshold will have their full content stored in external files with only a preview kept in context. Defaults to 2000."
        required: false
      group_token_threshold:
        type: integer
        description: "Maximum token count per compression group when using LLM-based compression. If None or 0, all messages are compressed in a single group. Messages exceeding this threshold individually will form their own group. Only used in 'compress' or 'auto' mode."
        required: false
      keep_recent_count:
        type: integer
        description: "Number of recent messages to preserve without compression or compaction. These messages remain unchanged to maintain conversation context. Defaults to 1 for compaction and 2 for compression."
        required: false
      store_dir:
        type: string
        description: "Directory path for storing summarized message content. Full tool message content and compressed message groups are saved as files in this directory. Required for compaction and compression operations."
        required: false
      chat_id:
        type: string
        description: "Unique identifier for the chat session, used for file naming when storing compressed message groups. If not provided, a UUID will be generated automatically."
        required: false

  grep_working_memory:
    flow_content: GrepOp()

  read_working_memory:
    flow_content: ReadFileOp()

  summary_working_memory_for_as:
    flow_content: MessageOffloadOp()
    description: "Working memory summary operation for AgentScope integration. Summarizes working memory by compacting tool messages and compressing conversation history without batch file writing. Same functionality as summary_working_memory but without the BatchWriteFileOp step."
    input_schema:
      messages:
        type: array
        description: "List of conversation messages to process for working memory summarization"
        required: true
      working_summary_mode:
        type: string
        description: "Working summary strategy: 'compact' only compacts large tool messages, 'compress' only applies LLM-based compression, 'auto' first compacts then optionally compresses when reduction is insufficient. Defaults to 'auto'."
        required: false
        enum: ["compact", "compress", "auto"]
      compact_ratio_threshold:
        type: number
        description: "Only used in 'auto' mode. Threshold for compaction ratio (tokens after compaction divided by original tokens). When the ratio is greater than this value, an additional LLM-based compression pass is triggered. Defaults to 0.75."
        required: false
      max_total_tokens:
        type: integer
        description: "Maximum token count threshold for triggering compression/compaction. For compaction, this is the total token count threshold. For compression, this excludes keep_recent_count messages and system messages. Defaults to 20000."
        required: false
      max_tool_message_tokens:
        type: integer
        description: "Maximum token count per tool message before compaction is applied. Tool messages exceeding this threshold will have their full content stored in external files with only a preview kept in context. Defaults to 2000."
        required: false
      group_token_threshold:
        type: integer
        description: "Maximum token count per compression group when using LLM-based compression. If None or 0, all messages are compressed in a single group. Messages exceeding this threshold individually will form their own group. Only used in 'compress' or 'auto' mode."
        required: false
      keep_recent_count:
        type: integer
        description: "Number of recent messages to preserve without compression or compaction. These messages remain unchanged to maintain conversation context. Defaults to 1 for compaction and 2 for compression."
        required: false
      store_dir:
        type: string
        description: "Directory path for storing summarized message content. Full tool message content and compressed message groups are saved as files in this directory. Required for compaction and compression operations."
        required: false
      chat_id:
        type: string
        description: "Unique identifier for the chat session, used for file naming when storing compressed message groups. If not provided, a UUID will be generated automatically."
        required: false


llm:
  default:
    backend: openai_compatible
    model_name: qwen3-30b-a3b-instruct-2507
    params:
      temperature: 0.6
    token_count: # Optional
      backend: base

  qwen3_coder_plus:
    backend: openai_compatible
    model_name: qwen3-coder-plus
    token_count: # Optional
      model_name: Qwen/Qwen3-Coder-480B-A35B-Instruct
      backend: hf
      params:
        use_mirror: true

  qwen3_coder_480b_instruct:
    backend: openai_compatible
    model_name: qwen3-coder-480b-a35b-instruct
    token_count: # Optional
      model_name: Qwen/Qwen3-Coder-480B-A35B-Instruct
      backend: hf
      params:
        use_mirror: true

  qwen3_coder_30b_instruct:
    backend: openai_compatible
    model_name: qwen3-coder-30b-a3b-instruct
    token_count: # Optional
      model_name: Qwen/Qwen3-Coder-30B-A3B-Instruct
      backend: hf
      params:
        use_mirror: true

  qwen3_30b_instruct:
    backend: openai_compatible
    model_name: qwen3-30b-a3b-instruct-2507
    token_count: # Optional
      model_name: Qwen/Qwen3-30B-A3B-Instruct-2507
      backend: hf
      params:
        use_mirror: true

  qwen3_30b_thinking:
    backend: openai_compatible
    model_name: qwen3-30b-a3b-thinking-2507
    token_count: # Optional
      model_name: Qwen/Qwen3-30B-A3B-Thinking-2507
      backend: hf
      params:
        use_mirror: true

  qwen3_235b_instruct:
    backend: openai_compatible
    model_name: qwen3-235b-a22b-instruct-2507
    token_count: # Optional
      model_name: Qwen/Qwen3-235B-A22B-Instruct-2507
      backend: hf
      params:
        use_mirror: true

  qwen3_235b_thinking:
    backend: openai_compatible
    model_name: qwen3-235b-a22b-thinking-2507
    token_count: # Optional
      model_name: Qwen/Qwen3-235B-A22B-Thinking-2507
      backend: hf
      params:
        use_mirror: true

  qwen3_80b_instruct:
    backend: openai_compatible
    model_name: qwen3-next-80b-a3b-instruct
    token_count: # Optional
      model_name: Qwen/Qwen3-Next-80B-A3B-Instruct
      backend: hf
      params:
        use_mirror: true

  qwen3_80b_thinking:
    backend: openai_compatible
    model_name: qwen3-next-80b-a3b-thinking
    token_count: # Optional
      model_name: Qwen/Qwen3-Next-80B-A3B-Thinking
      backend: hf
      params:
        use_mirror: true

  qwen3_max_instruct:
    backend: openai_compatible
    model_name: qwen3-max
    token_count: # Optional
      model_name: Qwen/Qwen3-Next-80B-A3B-Instruct  # NOTE: We use another model as a substitute.
      backend: hf
      params:
        use_mirror: true

  qwen25_max_instruct:
    backend: openai_compatible
    model_name: qwen-max-2025-01-25
    token_count: # Optional
      model_name: Qwen/Qwen2.5-72B-Instruct  # NOTE: We use another model as a substitute.
      backend: hf
      params:
        use_mirror: true

embedding_model:
  default:
    backend: openai_compatible
    model_name: text-embedding-v4
    params:
      dimensions: 1024

vector_store:
  default:
    backend: memory
    embedding_model: default
#    params:
#      hosts: "http://localhost:9200"
