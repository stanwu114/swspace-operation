backend: http
thread_pool_max_workers: 128

mcp:
  transport: sse
  host: "0.0.0.0"
  port: 8001

http:
  host: "0.0.0.0"
  port: 8002


flow:
  demo_http_flow:
    flow_content: GenSystemPromptOp() >> ChatOp()
    description: "ai chat assistant"  # Optional
    input_schema: # Optional
      query:
        type: string
        description: "user query"
        required: true

  demo_stream_http_flow:
    flow_content: GenSystemPromptOp() >> StreamChatOp()
    stream: true
    description: "ai chat assistant"  # Optional
    input_schema: # Optional
      query:
        type: string
        description: "user query"
        required: true

  demo_mcp_flow:
    flow_content: MockSearchOp()
    description: "search results for a given query."
    input_schema:
      query:
        type: string
        description: "user query"
        required: true

  skill_agent:
    flow_content: SkillAgentOp() << [LoadSkillOp(), ReadReferenceFileOp(), RunShellCommandOp()]

llm:
  default:
    backend: openai_compatible
    model_name: qwen3-30b-a3b-instruct-2507
    params:
      temperature: 0.6
    token_count: # Optional
      model_name: Qwen/Qwen3-30B-A3B-Instruct-2507
      backend: hf
      params:
        use_mirror: true

  qwen3_30b_instruct:
    backend: openai_compatible
    model_name: qwen3-30b-a3b-instruct-2507
    token_count: # Optional
      model_name: Qwen/Qwen3-30B-A3B-Instruct-2507
      backend: hf
      params:
        use_mirror: true

  qwen3_30b_thinking:
    backend: openai_compatible
    model_name: qwen3-30b-a3b-thinking-2507
    token_count: # Optional
      model_name: Qwen/Qwen3-30B-A3B-Thinking-2507
      backend: hf
      params:
        use_mirror: true

  qwen3_235b_instruct:
    backend: openai_compatible
    model_name: qwen3-235b-a22b-instruct-2507
    token_count: # Optional
      model_name: Qwen/Qwen3-235B-A22B-Instruct-2507
      backend: hf
      params:
        use_mirror: true

  qwen3_235b_thinking:
    backend: openai_compatible
    model_name: qwen3-235b-a22b-thinking-2507
    token_count: # Optional
      model_name: Qwen/Qwen3-235B-A22B-Thinking-2507
      backend: hf
      params:
        use_mirror: true

  qwen3_80b_instruct:
    backend: openai_compatible
    model_name: qwen3-next-80b-a3b-instruct
    token_count: # Optional
      model_name: Qwen/Qwen3-Next-80B-A3B-Instruct
      backend: hf
      params:
        use_mirror: true

  qwen3_80b_thinking:
    backend: openai_compatible
    model_name: qwen3-next-80b-a3b-thinking
    token_count: # Optional
      model_name: Qwen/Qwen3-Next-80B-A3B-Thinking
      backend: hf
      params:
        use_mirror: true

  qwen3_max_instruct:
    backend: openai_compatible
    model_name: qwen3-max
    token_count: # Optional
      model_name: Qwen/Qwen3-Next-80B-A3B-Instruct  # NOTE: We use another model as a substitute.
      backend: hf
      params:
        use_mirror: true

  qwen25_max_instruct:
    backend: openai_compatible
    model_name: qwen-max-2025-01-25
    token_count: # Optional
      model_name: Qwen/Qwen2.5-72B-Instruct  # NOTE: We use another model as a substitute.
      backend: hf
      params:
        use_mirror: true

embedding_model:
  default:
    backend: openai_compatible
    model_name: text-embedding-v4
    params:
      dimensions: 1024

vector_store:
  default:
#    backend: elasticsearch
    backend: memory
    embedding_model: default
#    params:
#      hosts: "http://localhost:9200"
